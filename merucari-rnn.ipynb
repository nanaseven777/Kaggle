{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7559,"databundleVersionId":44327,"sourceType":"competition"},{"sourceId":9967488,"sourceType":"datasetVersion","datasetId":6131706}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:57:14.571647Z","iopub.execute_input":"2024-11-21T08:57:14.572043Z","iopub.status.idle":"2024-11-21T08:57:14.600734Z","shell.execute_reply.started":"2024-11-21T08:57:14.572010Z","shell.execute_reply":"2024-11-21T08:57:14.599584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nfrom datetime import datetime\nstart_real= datetime.now()\n\ntrain_df = pd.read_table('/kaggle/input/dataset2/train.tsv')\ntest_df = pd.read_table('/kaggle/input/dataset2/test.tsv')\n\ntrain_df = train_df.drop(train_df[train_df['price'] < 3.0].index)\ntrain_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:57:14.602953Z","iopub.execute_input":"2024-11-21T08:57:14.603931Z","iopub.status.idle":"2024-11-21T08:57:30.759603Z","shell.execute_reply.started":"2024-11-21T08:57:14.603875Z","shell.execute_reply":"2024-11-21T08:57:30.758322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ndef wordCount(text):\n    try:\n        if text == 'No description yet':\n            return 0\n        else:\n            text = text.lower()\n            words = [w for w in text.split(\" \")]\n            return len(words)\n    except:\n        return 0\n\ntrain_df['name_len']=train_df['name'].apply(lambda x : wordCount(x))\ntest_df['name_len']=test_df['name'].apply(lambda x : wordCount(x))\n\ntrain_df['desc_len']=train_df['item_description'].apply(lambda x : wordCount(x))\ntest_df['desc_len']=test_df['item_description'].apply(lambda x : wordCount(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:57:30.761817Z","iopub.execute_input":"2024-11-21T08:57:30.762324Z","iopub.status.idle":"2024-11-21T08:57:41.681982Z","shell.execute_reply.started":"2024-11-21T08:57:30.762272Z","shell.execute_reply":"2024-11-21T08:57:41.680414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ntrain_df[\"target\"] = np.log1p(train_df.price)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:57:41.684523Z","iopub.execute_input":"2024-11-21T08:57:41.684927Z","iopub.status.idle":"2024-11-21T08:57:41.723366Z","shell.execute_reply.started":"2024-11-21T08:57:41.684891Z","shell.execute_reply":"2024-11-21T08:57:41.722103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ndef split_cat(text):\n    try:return text.split(\"/\")\n    except:return (\"No Label\",\"No Label\",\"No Label\")\n\ntrain_df['subcat_0'],train_df['subcat_1'],train_df['subcat_2'] = \\\n    zip(*train_df['category_name'].apply(lambda x : split_cat(x)))\n\ntest_df['subcat_0'],test_df['subcat_1'],test_df['subcat_2'] = \\\n    zip(*test_df['category_name'].apply(lambda x : split_cat(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:57:41.724747Z","iopub.execute_input":"2024-11-21T08:57:41.725109Z","iopub.status.idle":"2024-11-21T08:57:50.903654Z","shell.execute_reply.started":"2024-11-21T08:57:41.725076Z","shell.execute_reply":"2024-11-21T08:57:50.902356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nfull_set = pd.concat([train_df,test_df])\nall_brands = set(full_set['brand_name'].values)\n\ntrain_df['brand_name'] = train_df['brand_name'].fillna(value ='missing')\ntest_df['brand_name'] = test_df['brand_name'].fillna(value ='missing')\n\n#missingの数\ntrain_premissing = len(train_df.loc[train_df['brand_name']=='missing'])\ntest_premissing = len(test_df.loc[test_df['brand_name']=='missing'])\n\ndef brandfinder(line):\n    \"\"\"\n    line:ブランド名\n    ・missingを商品名に置き換える：\n        商品名がブランドリストの名前と完全に一致する場合\n    ・ブランド名を商品名に置き換える：\n        商品名がブランド名と完全一致した場合\n    ・ブランド名をそのままにする：\n        商品名がブランドリストの名前と一致しない\n        ブランド名がmissingだが商品名の単語がブランドリストと一致しない\n\n    \"\"\"\n    brand = line.iloc[0]\n    name = line.iloc[1]\n    namesplit = name.split(' ')\n\n    if brand == 'missing':\n        for x in namesplit:\n            if x in all_brands:\n                return name\n\n    if name in all_brands:#完全一致\n        return name\n\n    return brand\n\ntrain_df['brand_name'] = train_df[['brand_name','name']].apply(brandfinder,axis = 1)\ntest_df['brand_name'] = test_df[['brand_name','name']].apply(brandfinder,axis = 1)\n\ntrain_found = train_premissing-len(train_df.loc[train_df['brand_name'] == 'missing'])\nprint(train_premissing)\nprint(train_found)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:57:50.905338Z","iopub.execute_input":"2024-11-21T08:57:50.905842Z","iopub.status.idle":"2024-11-21T08:58:26.064452Z","shell.execute_reply.started":"2024-11-21T08:57:50.905790Z","shell.execute_reply":"2024-11-21T08:58:26.063249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nimport gc\n\ntrain_dfs,dev_dfs=train_test_split(\n    train_df,\n    random_state=123,\n    train_size=0.99,\n    test_size=0.01)\nn_devs = dev_dfs.shape[0]\nprint(n_devs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:58:26.065843Z","iopub.execute_input":"2024-11-21T08:58:26.066158Z","iopub.status.idle":"2024-11-21T08:58:28.306748Z","shell.execute_reply.started":"2024-11-21T08:58:26.066129Z","shell.execute_reply":"2024-11-21T08:58:28.305428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nfull_df = pd.concat([train_dfs,dev_dfs,test_df])\n\n#Nan値をmissingに置き換える\ndef fill_missing_values(df):\n    df.category_name = df.category_name.fillna(value='missing')\n    df.brand_name = df.brand_name.fillna(value='missing')\n    df.item_description = df.item_description.fillna(value='missing')\n\n    df.item_description = df.item_description.replace(\n        'No description yet','missing'\n    )\n    return df\n\nfull_df = fill_missing_values(full_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:58:28.308541Z","iopub.execute_input":"2024-11-21T08:58:28.308895Z","iopub.status.idle":"2024-11-21T08:58:31.072888Z","shell.execute_reply.started":"2024-11-21T08:58:28.308853Z","shell.execute_reply":"2024-11-21T08:58:31.071686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nle = LabelEncoder()\n\nle.fit(full_df.category_name)\nfull_df['category']=le.transform(full_df.category_name)\n\nle.fit(full_df.brand_name)\nfull_df.brand_name=le.transform(full_df.brand_name)\n\nle.fit(full_df.subcat_0)\nfull_df.subcat_0=le.transform(full_df.subcat_0)\n\nle.fit(full_df.subcat_1)\nfull_df.subcat_1=le.transform(full_df.subcat_1)\n\nle.fit(full_df.subcat_2)\nfull_df.subcat_2=le.transform(full_df.subcat_2)\n\ndel le\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:58:31.074245Z","iopub.execute_input":"2024-11-21T08:58:31.074620Z","iopub.status.idle":"2024-11-21T08:58:37.118966Z","shell.execute_reply.started":"2024-11-21T08:58:31.074584Z","shell.execute_reply":"2024-11-21T08:58:37.117938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nprint(\"transforming...\\n\")\nraw_text = np.hstack(\n    [full_df.item_description.str.lower(),\n    full_df.name.str.lower(),\n    full_df.category_name.str.lower()]\n)\n\nprint(raw_text.shape)\n\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\n\nprint('text for sequences\\n')\n\nfull_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df.item_description.str.lower())\nfull_df['seq_name'] = tok_raw.texts_to_sequences(full_df.name.str.lower())\n\ndel tok_raw\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:58:37.121630Z","iopub.execute_input":"2024-11-21T08:58:37.122025Z","iopub.status.idle":"2024-11-21T09:02:18.735321Z","shell.execute_reply.started":"2024-11-21T08:58:37.121990Z","shell.execute_reply":"2024-11-21T09:02:18.733987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#RNNモデルで使用する定数を定義\nMAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_CATEGORY_SEQ = 8\n\nMAX_TEXT = np.max([\n    np.max(full_df.seq_name.max()),\n    np.max(full_df.seq_item_description.max()),\n])+100\n\nMAX_CATEGORY = np.max(full_df.category.max())+1\nMAX_BRAND =np.max(full_df.brand_name.max())+1\nMAX_CONDITION = np.max(full_df.item_condition_id.max())+1\nMAX_DESC_LEN=np.max(full_df.desc_len.max())+1\nMAX_NAME_LEN =np.max(full_df.name_len.max())+1\n\nMAX_SUBCAT_0 =np.max(full_df.subcat_0.max())+1\nMAX_SUBCAT_1 =np.max(full_df.subcat_1.max())+1\nMAX_SUBCAT_2 =np.max(full_df.subcat_2.max())+1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T09:02:18.736803Z","iopub.execute_input":"2024-11-21T09:02:18.737718Z","iopub.status.idle":"2024-11-21T09:02:19.455242Z","shell.execute_reply.started":"2024-11-21T09:02:18.737662Z","shell.execute_reply":"2024-11-21T09:02:19.453950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndef get_rnn_data(dataset):\n    X ={\n        #商品ラベル\n        'name':pad_sequences(dataset.seq_name,maxlen=MAX_NAME_SEQ),\n        #商品説明ラベル\n        'item_desc':pad_sequences(dataset.seq_item_description,maxlen=MAX_ITEM_DESC_SEQ),\n        #ブランド名\n        'brand_name':np.array(dataset.brand_name),\n        #/区切りのカテゴリ名のラベル\n        'category':np.array(dataset.category),\n        #状態\n        'item_condition':np.array(dataset.item_condition_id),\n        #送料負担\n        'num_vars':np.array(dataset[['shipping']]),\n        #商品説明の単語数\n        'desc_len':np.array(dataset[[\"desc_len\"]]),\n        #商品名の単語数\n        'name_len':np.array(dataset[['name_len']]),\n        #カテゴリ0のラベル\n        'subcat_0':np.array(dataset.subcat_0),\n        'subcat_1':np.array(dataset.subcat_1),\n        'subcat_2':np.array(dataset.subcat_2)\n    }\n    return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T09:53:16.813724Z","iopub.execute_input":"2024-11-21T09:53:16.814141Z","iopub.status.idle":"2024-11-21T09:53:16.823755Z","shell.execute_reply.started":"2024-11-21T09:53:16.814103Z","shell.execute_reply":"2024-11-21T09:53:16.822500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_trains = train_dfs.shape[0]\nn_devs = dev_dfs.shape[0]\nn_tests = test_df.shape[0]\nprint(n_trains)\ntrain = full_df[:n_trains]\ndev =full_df[n_trains:n_trains+n_devs]\n\ntest = full_df[n_trains+n_devs:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T09:41:02.133466Z","iopub.execute_input":"2024-11-21T09:41:02.134414Z","iopub.status.idle":"2024-11-21T09:41:02.141832Z","shell.execute_reply.started":"2024-11-21T09:41:02.134368Z","shell.execute_reply":"2024-11-21T09:41:02.140633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = get_rnn_data(train)\n\nY_train = train.target.values.reshape(-1,1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T09:53:20.995345Z","iopub.execute_input":"2024-11-21T09:53:20.995726Z","iopub.status.idle":"2024-11-21T09:53:29.298190Z","shell.execute_reply.started":"2024-11-21T09:53:20.995695Z","shell.execute_reply":"2024-11-21T09:53:29.296973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_dev = get_rnn_data(dev)\nY_dev = dev.target.values.reshape(-1,1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T09:54:38.289689Z","iopub.execute_input":"2024-11-21T09:54:38.290102Z","iopub.status.idle":"2024-11-21T09:54:38.385473Z","shell.execute_reply.started":"2024-11-21T09:54:38.290065Z","shell.execute_reply":"2024-11-21T09:54:38.384354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = get_rnn_data(test)\n\ndel train_df\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T09:55:34.874605Z","iopub.execute_input":"2024-11-21T09:55:34.875030Z","iopub.status.idle":"2024-11-21T09:55:42.150546Z","shell.execute_reply.started":"2024-11-21T09:55:34.874993Z","shell.execute_reply":"2024-11-21T09:55:42.149400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input,Dropout,Dense,Embedding,Flatten\nfrom tensorflow.keras.layers import concatenate, GRU\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T10:00:24.403463Z","iopub.execute_input":"2024-11-21T10:00:24.403930Z","iopub.status.idle":"2024-11-21T10:00:24.422960Z","shell.execute_reply.started":"2024-11-21T10:00:24.403893Z","shell.execute_reply":"2024-11-21T10:00:24.421756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.random.seed(123)\n\ndef rmsle(Y,Y_pred):\n    assert Y.shape == Y_pred.shape\n    return np.sqrt(np.mean(np.square(Y_pred-Y)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T10:04:00.004186Z","iopub.execute_input":"2024-11-21T10:04:00.004658Z","iopub.status.idle":"2024-11-21T10:04:00.010723Z","shell.execute_reply.started":"2024-11-21T10:04:00.004620Z","shell.execute_reply":"2024-11-21T10:04:00.009350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def new_rnn_model(lr=0.001):\n    #lr:学習率、decay:学習減衰率\n    #入力層\n    name = Input(shape=[X_train[\"name\"].shape[1]],name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]],name=\"item_desc\")\n    brand_name = Input(shape=[1],name=\"brand_name\")\n    item_condition = Input(shape=[1],name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]],name=\"num_vars\")\n\n    name_len = Input(shape=[1],name=\"name_len\")\n    desc_len = Input(shape=[1],name=\"desc_len\")\n\n    subcat_0 = Input(shape=[1],name=\"subcat_0\")\n    subcat_1 = Input(shape=[1],name=\"subcat_1\")\n    subcat_2 = Input(shape=[1],name=\"subcat_2\")\n\n    #Embedding層\n    emb_name = Embedding(MAX_TEXT,20)(name)\n    emb_item_desc = Embedding(MAX_TEXT,60)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND,10)(brand_name)\n    emb_item_condition = Embedding(MAX_CONDITION,5)(item_condition)\n    emb_desc_len = Embedding(MAX_DESC_LEN,5)(desc_len)\n    emb_name_len = Embedding(MAX_NAME_LEN,5)(name_len)\n\n    emb_subcat_0 = Embedding(MAX_SUBCAT_0,10)(subcat_0)\n    emb_subcat_1 = Embedding(MAX_SUBCAT_1,10)(subcat_1)\n    emb_subcat_2 = Embedding(MAX_SUBCAT_2,10)(subcat_2)\n\n    #ReccurentユニットはGRUを使用\n    rnn_layer1 = GRU(16)(emb_item_desc)\n    rnn_layer2 = GRU(8)(emb_name)\n\n    #全結合層\n    main_1 = concatenate([\n        Flatten()(emb_brand_name),\n        Flatten()(emb_item_condition),\n        Flatten()(emb_desc_len),\n        Flatten()(emb_name_len),\n        Flatten()(emb_subcat_0),\n        Flatten()(emb_subcat_1),\n        Flatten()(emb_subcat_2),\n        rnn_layer1,\n        rnn_layer2,\n        num_vars#0か1なのでそのまま通す\n    ])\n    #512,256,128,64ユニットの層を追加\n    main_1 = Dropout(0.1)(\n        Dense(512,kernel_initializer='normal',activation='relu')(main_1))\n    main_1 = Dropout(0.1)(\n        Dense(256,kernel_initializer='normal',activation='relu')(main_1))\n    main_1 = Dropout(0.1)(\n        Dense(128,kernel_initializer='normal',activation='relu')(main_1))\n    main_1 = Dropout(0.1)(\n        Dense(64,kernel_initializer='normal',activation='relu')(main_1))\n\n    #出力層\n    output = Dense(1,activation='linear')(main_1)\n\n    #modelオブジェクトの作成\n    model = Model(\n        inputs=[name,item_desc,brand_name,item_condition,num_vars,\n               desc_len,name_len,subcat_0,subcat_1,subcat_2],\n        outputs=output\n    )\n\n    model.compile(loss='mse',optimizer=Adam(learning_rate=lr))\n\n    return model\n\nmodel = new_rnn_model()\nmodel.summary()\ndel model\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T10:53:30.750473Z","iopub.execute_input":"2024-11-21T10:53:30.750902Z","iopub.status.idle":"2024-11-21T10:53:35.391867Z","shell.execute_reply.started":"2024-11-21T10:53:30.750865Z","shell.execute_reply":"2024-11-21T10:53:35.390645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nBATCH_SIZE = 512*2\nepochs=3\n\n#学習減衰率\nexp_decay = lambda init,fin,steps:(init/fin)**(1/(steps-1))-1\nsteps = int(len(X_train[\"name\"])/BATCH_SIZE)* epochs\nlr_init =0.005\nlr_fin= 0.001\nlr_decay = exp_decay(lr_init,lr_fin,steps)\n\n#モデルを生成\nrnn_model = new_rnn_model(lr=lr,decay=lr_decay)\n#学習\nrnn_model.fit(X_train,Y_train,\n             epochs=epochs,\n             batch_size=BATCH_SIZE,\n             validation_data=(X_dev,Y_dev),\n             verbose=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\n# バッチサイズとエポック数\nBATCH_SIZE = 512 * 2\nepochs = 3\n\n# 学習減衰率の計算\ndef exp_decay_schedule(epoch, lr):\n    return lr * lr_decay\n\nexp_decay = lambda init, fin, steps: (init / fin) ** (1 / (steps - 1)) - 1\nsteps = int(len(X_train[\"name\"]) / BATCH_SIZE) * epochs\nlr_init = 0.005  # 初期学習率\nlr_fin = 0.001   # 最終学習率\nlr_decay = exp_decay(lr_init, lr_fin, steps)\n\n# モデルを生成\nrnn_model = new_rnn_model(lr=lr_init)  # 初期学習率を設定\n\n# 学習率スケジューラーのコールバック\nlr_scheduler = LearningRateScheduler(exp_decay_schedule)\n\n# モデルの学習\nrnn_model.fit(\n    X_train,\n    Y_train,\n    epochs=epochs,\n    batch_size=BATCH_SIZE,\n    validation_data=(X_dev, Y_dev),\n    verbose=1,\n    callbacks=[lr_scheduler]  # スケジューラーを渡す\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:39:17.977877Z","iopub.execute_input":"2024-11-21T11:39:17.978988Z","iopub.status.idle":"2024-11-21T12:02:31.596097Z","shell.execute_reply.started":"2024-11-21T11:39:17.978944Z","shell.execute_reply":"2024-11-21T12:02:31.594612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y_dev_preds_rnn = rnn_model.predict(X_dev,batch_size=BATCH_SIZE)\n\nprint(rmsle(Y_dev,Y_dev_preds_rnn))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:44:46.006613Z","iopub.execute_input":"2024-11-21T12:44:46.007065Z","iopub.status.idle":"2024-11-21T12:44:48.051945Z","shell.execute_reply.started":"2024-11-21T12:44:46.007028Z","shell.execute_reply":"2024-11-21T12:44:48.050619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rnn_preds = rnn_model.predict(X_test,batch_size=BATCH_SIZE,verbose=1)\nrnn_preds=np.expm1(rnn_preds)\n\ndel rnn_model\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T13:28:17.771648Z","iopub.execute_input":"2024-11-21T13:28:17.772949Z","iopub.status.idle":"2024-11-21T13:28:55.424757Z","shell.execute_reply.started":"2024-11-21T13:28:17.772895Z","shell.execute_reply":"2024-11-21T13:28:55.423446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}